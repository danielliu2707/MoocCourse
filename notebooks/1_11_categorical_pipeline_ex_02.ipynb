{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Exercise M1.05\n",
    "\n",
    "The goal of this exercise is to evaluate the impact of feature preprocessing\n",
    "on a pipeline that uses a decision-tree-based classifier instead of a logistic\n",
    "regression.\n",
    "\n",
    "- The first question is to empirically evaluate whether scaling numerical\n",
    "  features is helpful or not;\n",
    "- The second question is to evaluate whether it is empirically better (both\n",
    "  from a computational and a statistical perspective) to use integer coded or\n",
    "  one-hot encoded categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv(\"../datasets/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "data = adult_census.drop(columns=[target_name, \"education-num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous notebooks, we use the utility `make_column_selector` to\n",
    "select only columns with a specific data type. Besides, we list in advance all\n",
    "categories for the categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c6d076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "# define selector\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# define column names for each dtype\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56a64a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# define transformers\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "numerical_preprocessor = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference pipeline (no numerical scaling and integer-coded categories)\n",
    "\n",
    "First let's time the pipeline we used in the main notebook to serve as a\n",
    "reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross-validation accuracy is: 0.873 ¬± 0.002 with a fitting time of 2.841\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "categorical_preprocessor = OrdinalEncoder(\n",
    "    handle_unknown=\"use_encoded_value\", unknown_value=-1\n",
    ")\n",
    "preprocessor = ColumnTransformer(\n",
    "    [(\"categorical\", categorical_preprocessor, categorical_columns)],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())\n",
    "\n",
    "start = time.time()\n",
    "cv_results = cross_validate(model, data, target)\n",
    "elapsed_time = time.time() - start\n",
    "\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(\n",
    "    \"The mean cross-validation accuracy is: \"\n",
    "    f\"{scores.mean():.3f} ¬± {scores.std():.3f} \"\n",
    "    f\"with a fitting time of {elapsed_time:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling numerical features\n",
    "\n",
    "Let's write a similar pipeline that also scales the numerical features using\n",
    "`StandardScaler` (or similar):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross-validation accuracy is: 0.873 ¬± 0.002 with a fitting time of 2.902\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "import time\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "data = adult_census.drop(columns=[target_name, \"education-num\"])\n",
    "\n",
    "# define selector\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# define column names for each dtype\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "\n",
    "# define transformers\n",
    "categorical_preprocessor = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "# define column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"ordinal-encoder\", categorical_preprocessor, categorical_columns),\n",
    "        (\"standard-scaler\", numerical_preprocessor, numerical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# make pipeline\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())\n",
    "\n",
    "# running cv on entire dataset to estimate generalisability\n",
    "start_time = time.time()\n",
    "cv_results = cross_validate(model, data, target, cv=5)\n",
    "elapsed_time = time.time() - start_time\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(\n",
    "    \"The mean cross-validation accuracy is: \"\n",
    "    f\"{scores.mean():.3f} ¬± {scores.std():.3f} \"\n",
    "    f\"with a fitting time of {elapsed_time:.3f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa37cc5",
   "metadata": {},
   "source": [
    "There was no improvement in model performance, and a slight worsening of computation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of categorical variables\n",
    "\n",
    "We observed that integer coding of categorical variables can be very\n",
    "detrimental for linear models. However, it does not seem to be the case for\n",
    "`HistGradientBoostingClassifier` models, as the cross-validation score of the\n",
    "reference pipeline with `OrdinalEncoder` is reasonably good.\n",
    "\n",
    "Let's see if we can get an even better accuracy with `OneHotEncoder`.\n",
    "\n",
    "Hint: `HistGradientBoostingClassifier` does not yet support sparse input data.\n",
    "You might want to use `OneHotEncoder(handle_unknown=\"ignore\",\n",
    "sparse_output=False)` to force the use of a dense representation as a\n",
    "workaround."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean cross-validation accuracy is: 0.873 ¬± 0.002 with a fitting time of 6.386\n"
     ]
    }
   ],
   "source": [
    "# Write your code here.\n",
    "# Write your code here.\n",
    "import time\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "data = adult_census.drop(columns=[target_name, \"education-num\"])\n",
    "\n",
    "# define selector\n",
    "numerical_columns_selector = selector(dtype_exclude=object)\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "\n",
    "# define column names for each dtype\n",
    "numerical_columns = numerical_columns_selector(data)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "\n",
    "# define transformers\n",
    "categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "numerical_preprocessor = StandardScaler()\n",
    "\n",
    "# define column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"ordinal-encoder\", categorical_preprocessor, categorical_columns),\n",
    "        (\"standard-scaler\", numerical_preprocessor, numerical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# make pipeline\n",
    "model = make_pipeline(preprocessor, HistGradientBoostingClassifier())\n",
    "\n",
    "# running cv on entire dataset to estimate generalisability\n",
    "start_time = time.time()\n",
    "cv_results = cross_validate(model, data, target, cv=5)\n",
    "elapsed_time = time.time() - start_time\n",
    "scores = cv_results[\"test_score\"]\n",
    "\n",
    "print(\n",
    "    \"The mean cross-validation accuracy is: \"\n",
    "    f\"{scores.mean():.3f} ¬± {scores.std():.3f} \"\n",
    "    f\"with a fitting time of {elapsed_time:.3f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca679c2",
   "metadata": {},
   "source": [
    "Model accuracy is identical. Computational performance worsens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "From an accuracy point of view, the result is almost exactly the same. The\n",
    "reason is that `HistGradientBoostingClassifier` is expressive and robust\n",
    "enough to deal with misleading ordering of integer coded categories (which was\n",
    "not the case for linear models).\n",
    "\n",
    "However from a computation point of view, the training time is much longer (many more features):\n",
    "this is caused by the fact that `OneHotEncoder` generates more features than\n",
    "`OrdinalEncoder`; for each unique categorical value a column is created.\n",
    "\n",
    "Note that the current implementation `HistGradientBoostingClassifier` is still\n",
    "incomplete, and once sparse representation are handled correctly, training\n",
    "time might improve with such kinds of encodings.\n",
    "\n",
    "The main take away message is that arbitrary integer coding of categories is\n",
    "perfectly fine for `HistGradientBoostingClassifier` and yields fast training\n",
    "times.\n",
    "\n",
    "Which encoder should I use?\n",
    "\n",
    "|                  | Meaningful order              | Non-meaningful order |\n",
    "| ---------------- | ----------------------------- | -------------------- |\n",
    "| Tree-based model | `OrdinalEncoder`              | `OrdinalEncoder` with reasonable depth    |\n",
    "| Linear model     | `OrdinalEncoder` with caution | `OneHotEncoder`      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"admonition important alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Important</p>\n",
    "<ul class=\"last simple\">\n",
    "<li><tt class=\"docutils literal\">OneHotEncoder</tt>: always does something meaningful, but can be unnecessary\n",
    "slow with trees.</li>\n",
    "<li><tt class=\"docutils literal\">OrdinalEncoder</tt>: can be detrimental for linear models unless your category\n",
    "has a meaningful order and you make sure that <tt class=\"docutils literal\">OrdinalEncoder</tt> respects this\n",
    "order. Trees can deal with <tt class=\"docutils literal\">OrdinalEncoder</tt> fine as long as they are deep\n",
    "enough. However, when you allow the decision tree to grow very deep, it might\n",
    "overfit on other features.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to one-hot-encoding and ordinal encoding categorical features,\n",
    "scikit-learn offers the [`TargetEncoder`](https://scikit-learn.org/stable/modules/preprocessing.html#target-encoder).\n",
    "This encoder is well suited for nominal, categorical features with high\n",
    "cardinality. This encoding strategy is beyond the scope of this course,\n",
    "but the interested reader is encouraged to explore this encoder."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "mlmooc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
